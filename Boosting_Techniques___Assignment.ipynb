{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Boosting Techniques**"
      ],
      "metadata": {
        "id": "nk1BX7gMYVQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is Boosting in Machine Learning? Explain how it improves weak learners.**\n",
        "\n",
        "* Boosting is an ensemble learning technique in machine learning that combines multiple weak learners to form a strong learner. A weak learner is a model that performs just slightly better than random guessing (e.g., a shallow decision tree). The idea of boosting is to sequentially train weak learners, each one trying to correct the errors of its predecessor.\n",
        "\n",
        "**How Boosting Improves Weak Learners**\n",
        "\n",
        " Here's how boosting improves performance:\n",
        "\n",
        "**1. Initial Training:**\n",
        "\n",
        "* The first weak learner is trained on the data.\n",
        "\n",
        "**2. Error Focus:**\n",
        "\n",
        "* After the first learner is trained, Boosting identifies the errors (misclassified instances).\n",
        "\n",
        "* It assigns higher weights to these misclassified examples.\n",
        "\n",
        "**3. Next Learners Focus on Mistakes:**\n",
        "\n",
        "* The next weak learner is trained on the re-weighted data, focusing more on difficult cases.\n",
        "\n",
        "**4. Repeat:**\n",
        "\n",
        "* This process continues for a number of iterations.\n",
        "\n",
        "**5. Combine Learners:**\n",
        "\n",
        "* Final prediction is made by combining all the weak learners, usually by weighted majority vote (classification) or weighted sum (regression)."
      ],
      "metadata": {
        "id": "J1ccdLL4YbnZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?**\n",
        "\n",
        "\n",
        "Let’s break down the key differences:\n",
        "\n",
        "**AdaBoost**\n",
        "\n",
        "* **Main Idea:**\tFocuses on misclassified examples by adjusting sample weights\n",
        "\n",
        "* **How next learner is trained:**\tRe-weights the training data: misclassified samples get higher weights, so the next learner focuses on them\n",
        "\n",
        "* **Error Correction:**\tLearner focuses on hard-to-classify points based on previous model’s errors\n",
        "\n",
        "* **Final Prediction:**\tWeighted majority vote (classification) or weighted sum (regression) of all learners\n",
        "\n",
        "* **Loss Function:**\tExponential loss (default)\n",
        "\n",
        "* **Model Flexibility:**\tLess flexible—mostly for classification\n",
        "\n",
        "**Gradient Boosting**\n",
        "\n",
        "* **Main Idea:** Fits new learners to the residual errors (i.e., gradients) of the current model\n",
        "\n",
        "* **How next learner is trained:** Trains the next learner to predict the gradient of the loss function (i.e., how to reduce the current error)\n",
        "\n",
        "* **Error Correction:** Learner focuses on reducing the loss function directly via gradient descent\n",
        "\n",
        "* **Final Prediction:** Additive model: sum of all learners’ predictions\n",
        "\n",
        "* **Loss Function:**\tCan use different loss functions (e.g., MSE, MAE, log-loss), depending on the problem\n",
        "\n",
        "* **Model Flexibility:**  \tMore flexible—works for classification and regression, and supports custom loss functions"
      ],
      "metadata": {
        "id": "D7_-dl5xe_h5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: How does regularization help in XGBoost?**\n",
        "\n",
        "**Type**.............................................\t**Parameter**........................................\t**Effect**\n",
        "\n",
        "**L2 Regularization**...........................\tlambda.........................\tShrinks leaf weights → smoother predictions, less overfitting.\n",
        "\n",
        "**L1 Regularization**.............................\talpha........................\tEncourages sparsity → some leaf weights go to 0.\n",
        "\n",
        "**Tree Complexity Penalty**...................\tgamma..........................\tDiscourages adding too many leaves → simpler trees."
      ],
      "metadata": {
        "id": "pQfIPGtDoZ5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: Why is CatBoost considered efficient for handling categorical data?**\n",
        "\n",
        "**1. Native Support for Categorical Features**\n",
        "\n",
        "* CatBoost natively supports categorical variables without requiring manual preprocessing (like one-hot or label encoding). You can directly pass categorical columns by specifying them, and CatBoost will internally transform them.\n",
        "\n",
        "\n",
        "**2. Target-Based Statistics with Ordered Boosting**\n",
        "\n",
        "* CatBoost uses a technique called \"ordered target statistics\" (or ordered boosting) to convert categorical features into numerical representations. Here's how it works:\n",
        "\n",
        "* It computes the mean target value for each category, but to avoid target leakage, it only uses past data points (not the whole dataset).\n",
        "\n",
        "* This is done by permuting the dataset and using statistics from earlier samples when encoding each categorical value.\n",
        "\n",
        "* This technique ensures that the encoded value of a category is not influenced by the label of the current observation — a major problem in naïve target encoding.\n",
        "\n",
        "**3. Avoidance of Overfitting**\n",
        "\n",
        "* Because of the ordered boosting technique and correct handling of target statistics, CatBoost significantly reduces overfitting that often occurs when dealing with high-cardinality categorical variables.\n",
        "\n",
        "**4. Efficient Handling of High-Cardinality Categories**\n",
        "\n",
        "* CatBoost handles high-cardinality categorical features more efficiently than one-hot encoding or label encoding. Instead of exploding the feature space or imposing arbitrary order, it builds robust numeric representations based on historical data patterns.\n",
        "\n",
        "**5. Automatic Combination of Categorical Features**\n",
        "\n",
        "* CatBoost can automatically detect and create combinations of categorical features (like \"city\" + \"device_type\") if they improve the model’s accuracy, without manual feature engineering.\n",
        "\n",
        "**6. Speed and Accuracy**\n",
        "\n",
        "* Despite these complex transformations, CatBoost is still competitive in training time and inference speed. Its optimized C++ backend allows it to run efficiently even with many categorical columns."
      ],
      "metadata": {
        "id": "UDVj-oUrsODW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: What are some real-world applications where boosting techniques are preferred over bagging methods?**\n",
        "\n",
        "Here are several real-world application areas where boosting is typically favored:\n",
        "\n",
        "**1. Credit Scoring & Risk Modeling (Finance)**\n",
        "\n",
        "* **Use Case:** Predicting loan defaults, credit card fraud, or customer creditworthiness.\n",
        "\n",
        "* **Why Boosting?** Boosting models (like XGBoost) capture complex non-linear relationships and often outperform Random Forests in AUC/ROC and log-loss, which are critical for risk management.\n",
        "\n",
        "**2. Customer Churn Prediction (Telecom, SaaS, Retail)**\n",
        "\n",
        "* **Use Case:** Identifying customers likely to leave a service.\n",
        "\n",
        "* **Why Boosting?** Boosting algorithms can handle imbalanced datasets well and achieve better precision-recall performance, especially in binary classification tasks.\n",
        "\n",
        "**3. Click-Through Rate (CTR) Prediction (Online Advertising, Recommendation Systems)**\n",
        "\n",
        "* **Use Case:** Predicting whether a user will click an ad or recommended item.\n",
        "\n",
        "* **Why Boosting?** Gradient boosting (particularly CatBoost) handles high-cardinality categorical data (like user IDs, item IDs) extremely well, offering state-of-the-art performance in CTR prediction.\n",
        "\n",
        "**4. Medical Diagnosis & Risk Prediction**\n",
        "\n",
        "* **Use Case:** Predicting disease risk, hospital readmissions, or treatment outcomes.\n",
        "\n",
        "* **Why Boosting?** In healthcare, high accuracy and interpretability (e.g., SHAP values for explainability) make boosting models highly suitable.\n",
        "\n",
        "**5. Kaggle Competitions & ML Benchmarks**\n",
        "\n",
        "* **Use Case:** Structured/tabular data challenges (e.g., Titanic survival prediction, sales forecasting).\n",
        "\n",
        "* **Why Boosting?** Gradient boosting models frequently top leaderboard results due to their superior handling of structured features and ensemble power.\n",
        "\n",
        "**6. Fraud Detection (Banking, E-commerce)**\n",
        "\n",
        "* **Use Case:** Detecting fraudulent transactions in real time.\n",
        "\n",
        "* **Why Boosting?** Boosting models like XGBoost and LightGBM can learn subtle patterns and interactions in features that help identify rare fraudulent events with high precision.\n",
        "\n",
        "**7. Insurance Claim Prediction**\n",
        "\n",
        "* **Use Case:** Predicting whether a customer will make a claim and estimating claim amounts.\n",
        "\n",
        "* **Why Boosting?** Boosting models are adept at modeling heteroskedasticity and skewed distributions commonly found in claim amounts.\n",
        "\n",
        "**8. Demand Forecasting (Retail, Supply Chain)**\n",
        "\n",
        "* **Use Case:** Predicting product demand at different locations.\n",
        "\n",
        "* **Why Boosting?** Able to model seasonality, trends, and non-linear interactions, boosting models often outperform linear and bagging models in time-series tabular tasks."
      ],
      "metadata": {
        "id": "Eo0Y1LldxDCN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6: Write a Python program to:**\n",
        "\n",
        "● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "\n",
        "● Print the model accuracy\n"
      ],
      "metadata": {
        "id": "LJFk4B7izThe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the AdaBoost classifier\n",
        "model = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVA9SOU30Ba0",
        "outputId": "abbe05d0-f151-446b-b301-b9d75c3edd3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9649122807017544\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.93      0.95        43\n",
            "           1       0.96      0.99      0.97        71\n",
            "\n",
            "    accuracy                           0.96       114\n",
            "   macro avg       0.97      0.96      0.96       114\n",
            "weighted avg       0.97      0.96      0.96       114\n",
            "\n",
            "Confusion Matrix:\n",
            " [[40  3]\n",
            " [ 1 70]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "● **Print the model accuracy**"
      ],
      "metadata": {
        "id": "1w5Y8P_S0Rap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the AdaBoost Classifier\n",
        "model = AdaBoostClassifier(n_estimators=50, learning_rate=1.0, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMeih4gd0WQG",
        "outputId": "576f24f3-1c9f-40d7-b7cd-867bcdc73806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7: Write a Python program to:**\n",
        "\n",
        "● Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "\n",
        "● Evaluate performance using R-squared score"
      ],
      "metadata": {
        "id": "ttgWzD1m0rRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize and train the Gradient Boosting Regressor\n",
        "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model using R² score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"R² Score (Model Accuracy):\", r2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHGBrFMX01vN",
        "outputId": "d46c94d8-adb1-4fee-d3bc-bb2e107725fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R² Score (Model Accuracy): 0.7756446042829697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Evaluate performance using R-squared score**"
      ],
      "metadata": {
        "id": "dSAzUB4R1BXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split the dataset (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Train the Gradient Boosting Regressor\n",
        "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate performance using R² score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"R² Score:\", r2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BSteaEs1Fa6",
        "outputId": "bfd54172-58ec-4fac-96d5-bf304306eaa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R² Score: 0.7756446042829697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: Write a Python program to:**\n",
        "\n",
        "● Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "\n",
        "● Tune the learning rate using GridSearchCV\n",
        "\n",
        "● Print the best parameters and accuracy\n"
      ],
      "metadata": {
        "id": "IqsP2ti51Qzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split the dataset into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Initialize and train the XGBoost Classifier\n",
        "model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"XGBoost Classifier Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5XBrT6_1cDL",
        "outputId": "c197b61e-d5e2-41e0-9cc3-f2c110e7cd45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Classifier Accuracy: 0.956140350877193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [18:26:59] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tune the learning rate using GridSearchCV**"
      ],
      "metadata": {
        "id": "ytK5fiXE1d8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Define the XGBoost classifier\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Step 4: Define the parameter grid (tuning learning_rate)\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]\n",
        "}\n",
        "\n",
        "# Step 5: Perform GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=xgb_model,\n",
        "                           param_grid=param_grid,\n",
        "                           cv=5,\n",
        "                           scoring='accuracy',\n",
        "                           verbose=1,\n",
        "                           n_jobs=-1)\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Evaluate the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 7: Print results\n",
        "print(\"Best Learning Rate:\", grid_search.best_params_['learning_rate'])\n",
        "print(\"Best Cross-Validated Accuracy:\", grid_search.best_score_)\n",
        "print(\"Test Accuracy with Best Model:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EorHitM_1uzh",
        "outputId": "bef93ff1-b8e5-43d4-ca3a-799e1e192f2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
            "Best Learning Rate: 0.2\n",
            "Best Cross-Validated Accuracy: 0.9670329670329672\n",
            "Test Accuracy with Best Model: 0.956140350877193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [18:28:22] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Print the best parameters and accuracy**"
      ],
      "metadata": {
        "id": "4B4YllZD17K2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Define the XGBoost classifier\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Step 4: Define parameter grid to tune the learning rate\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]\n",
        "}\n",
        "\n",
        "# Step 5: Set up GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=xgb,\n",
        "    param_grid=param_grid,\n",
        "    scoring='accuracy',\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Step 6: Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Predict and evaluate\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 8: Print best parameters and accuracy\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n",
        "print(\"Test Accuracy with Best Model:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSXjsj8T18jq",
        "outputId": "a6bdfed4-7c81-4909-f4f0-5c9b6a2305e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
            "Best Parameters: {'learning_rate': 0.2}\n",
            "Best Cross-Validation Accuracy: 0.9670329670329672\n",
            "Test Accuracy with Best Model: 0.956140350877193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [18:29:20] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Write a Python program to:**\n",
        "\n",
        "● Train a CatBoost Classifier\n",
        "\n",
        "● Plot the confusion matrix using seaborn\n"
      ],
      "metadata": {
        "id": "5L_X2Zxp2DEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If CatBoost is not installed, uncomment the next line to install it:\n",
        "# !pip install catboost\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize CatBoostClassifier\n",
        "model = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, verbose=0, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "FY9Wg77y5QOt",
        "outputId": "2d655cad-d7d6-463f-8746-ebb638e92e66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'catboost'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4288057657.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# !pip install catboost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_breast_cancer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'catboost'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot the confusion matrix using seaborn**"
      ],
      "metadata": {
        "id": "Ma4iqLmp2M0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load data\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train AdaBoost classifier\n",
        "model = AdaBoostClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix with seaborn heatmap\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
        "            xticklabels=data.target_names, yticklabels=data.target_names)\n",
        "plt.xlabel(\"Predicted label\")\n",
        "plt.ylabel(\"True label\")\n",
        "plt.title(\"Confusion Matrix - AdaBoost Classifier\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "xGj7nhHr3K4a",
        "outputId": "b27c1197-378e-4499-9da7-aa77213ecbf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: You're working for a FinTech company trying to predict loan default using customer demographics and transaction behavior.**\n",
        "\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and categorical features.\n",
        "\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "\n",
        "● Data preprocessing & handling missing/categorical values\n",
        "\n",
        "● Choice between AdaBoost, XGBoost, or CatBoost\n",
        "\n",
        "● Hyperparameter tuning strategy\n",
        "\n",
        "● Evaluation metrics you'd choose and why\n",
        "\n",
        "● How the business would benefit from your model"
      ],
      "metadata": {
        "id": "tcyscAeR5a8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data preprocessing & handling missing/categorical values**\n",
        "\n",
        "\n",
        "**Step 1: Understanding the Problem and Dataset**\n",
        "\n",
        "* **Objective:** Predict loan default (binary classification).\n",
        "\n",
        "* **Challenges:**\n",
        "\n",
        " * Imbalanced target (defaults are usually minority).\n",
        "\n",
        " * Missing values scattered across features.\n",
        "\n",
        " * Mixed data types: numerical + categorical.\n",
        "\n",
        " * Potential outliers and noisy data.\n",
        "\n",
        "**Step 2: Data Collection & Initial Exploration**\n",
        "\n",
        "* Load the data, get basic stats (.info(), .describe(), missing value counts).\n",
        "\n",
        "* Visualize class distribution to confirm imbalance.\n",
        "\n",
        "* Identify categorical vs numeric features.\n",
        "\n",
        "* Analyze missing value patterns (random or systematic).\n",
        "\n",
        "**Step 3: Data Cleaning & Preprocessing**\n",
        "\n",
        "**a) Handling Missing Values**\n",
        "\n",
        "* **Numerical features:**\n",
        "\n",
        " * Simple imputation (median) or advanced methods like KNN or iterative imputation.\n",
        "\n",
        " * Alternatively, boosting frameworks like CatBoost can natively handle missing values, so minimal imputation may suffice.\n",
        "\n",
        "**Categorical features:**\n",
        "\n",
        "* Fill missing with a special category like \"Missing\" or \"Unknown\".\n",
        "\n",
        "* Avoid dropping rows if missingness is informative.\n",
        "\n",
        "**b) Encoding Categorical Variables**\n",
        "\n",
        "* Boosting algorithms vary:\n",
        "\n",
        " * **CatBoost:** can directly handle categorical features, so just specify categorical feature indices.\n",
        "\n",
        " * **LightGBM / XGBoost:** encode categoricals as label-encoded integers or use target encoding.\n",
        "\n",
        "* Avoid one-hot encoding for high-cardinality features due to dimensionality explosion.\n",
        "\n",
        "**Step 4: Feature Engineering**\n",
        "\n",
        "* Create derived features, e.g.:\n",
        "\n",
        "  * Ratios (debt-to-income), counts (number of transactions), temporal features.\n",
        "\n",
        "* Detect and treat outliers if necessary.\n",
        "\n",
        "* Scale numeric features only if algorithm benefits (generally boosting methods don’t require scaling).\n",
        "\n",
        "**Step 5: Handling Imbalanced Dataset**\n",
        "\n",
        "* Choose strategies to mitigate imbalance:\n",
        "\n",
        " * **Resampling:** oversample minority (SMOTE) or undersample majority class.\n",
        "\n",
        " * **Algorithmic:** Use boosting model’s built-in class weights or scale_pos_weight.\n",
        "\n",
        " * **Evaluation metrics:** Use metrics suited for imbalance — AUC-ROC, precision-recall curve, F1-score rather than accuracy.\n",
        "\n",
        "**Step 6: Splitting Data**\n",
        "\n",
        "* Split into training and test sets (e.g., 80/20).\n",
        "\n",
        "* Consider stratified splitting to keep class distribution.\n",
        "\n",
        "**Step 7: Model Selection & Training with Boosting Techniques**\n",
        "\n",
        "**a) Choose the model**\n",
        "\n",
        " * CatBoostClassifier or LightGBM/XGBoost with categorical support.\n",
        "\n",
        "**b) Specify categorical features explicitly (if using CatBoost)**\n",
        "\n",
        "* Pass categorical feature indices for native handling.\n",
        "\n",
        "**c) Tune hyperparameters:**\n",
        "\n",
        "* iterations, learning_rate, depth, l2_leaf_reg, class_weights (for CatBoost), scale_pos_weight (for LightGBM/XGBoost).\n",
        "\n",
        "**Step 8: Model Evaluation**\n",
        "\n",
        "* Use metrics that reflect imbalance:\n",
        "\n",
        " * ROC AUC\n",
        "\n",
        " * Precision, Recall, F1-score\n",
        "\n",
        " * Confusion matrix\n",
        "\n",
        "* Perform cross-validation (stratified) to validate performance stability.\n",
        "\n",
        "**Step 9: Model Interpretation & Explainability**\n",
        "\n",
        "* Use SHAP or CatBoost’s built-in feature importance.\n",
        "\n",
        "* Analyze which features drive predictions, particularly for loan default.\n",
        "\n",
        "**Step 10: Deployment & Monitoring**\n",
        "\n",
        "* Prepare data pipeline with the same preprocessing steps.\n",
        "\n",
        "* Monitor model drift and retrain as needed."
      ],
      "metadata": {
        "id": "T0wXjBPD59mu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Choice between AdaBoost, XGBoost, or CatBoost**\n",
        "\n",
        "\n",
        "**1. Problem Understanding & Business Goal**\n",
        "\n",
        "**Objective:** Predict loan default (binary classification)\n",
        "\n",
        "**Goal:** Maximize predictive accuracy (e.g., AUC-ROC, F1-score) while minimizing false negatives (defaults incorrectly predicted as non-defaults)\n",
        "\n",
        "**Challenge:** Imbalanced data, missing values, mixed feature types\n",
        "\n",
        "**2. Data Preprocessing**\n",
        "\n",
        "2.1 Handle Missing Values\n",
        "\n",
        "* Numeric: Impute using median (robust to outliers)\n",
        "\n",
        "* Categorical:\n",
        "\n",
        " * Low missingness: Use mode\n",
        "\n",
        " * High missingness: Create a separate \"Missing\" category or drop if irrelevant\n",
        "\n",
        "* Or better: Use models (e.g., CatBoost) that handle missing values natively\n",
        "\n",
        "**2.2 Encode Categorical Features**\n",
        "\n",
        "* Option 1: One-hot encoding (for models that require numerical input, e.g., AdaBoost, XGBoost)\n",
        "\n",
        "* Option 2: Leave as-is for CatBoost (which handles categorical features natively)\n",
        "\n",
        "* Avoid high cardinality one-hot explosion\n",
        "\n",
        "**2.3 Feature Scaling**\n",
        "\n",
        "* Boosting trees don’t require feature scaling — skip for XGBoost or CatBoost\n",
        "\n",
        "**3. Address Class Imbalance**\n",
        "\n",
        "Choose one or more of the following:\n",
        "\n",
        "* Resampling: SMOTE (oversampling) or RandomUnderSampler\n",
        "\n",
        "* Class weights: Use built-in parameters in the model\n",
        "\n",
        " * scale_pos_weight (XGBoost)\n",
        "\n",
        "* class_weights (CatBoost)\n",
        "\n",
        "* Evaluation metric: Use metrics that handle imbalance (AUC, F1, precision-recall)\n",
        "\n",
        "**4. Exploratory Data Analysis (EDA)**\n",
        "\n",
        "* Analyze distribution of features\n",
        "\n",
        "* Investigate correlations\n",
        "\n",
        "* Examine relationships between features and default rates\n",
        "\n",
        "* Detect outliers\n",
        "\n",
        "**5. Model Selection: Boosting Technique**\n",
        "\n",
        " **CatBoost — Best Choice**\n",
        "\n",
        "**Why CatBoost:**\n",
        "\n",
        "* Handles categorical features natively\n",
        "\n",
        "* Handles missing values automatically\n",
        "\n",
        "* Built-in support for imbalanced classification via class_weights\n",
        "\n",
        "* Great default performance with minimal tuning\n",
        "\n",
        "* Robust to overfitting due to Ordered Boosting\n",
        "\n",
        "* Easy to implement and faster than you might expect\n",
        "\n",
        "**Alternative Options:**\n",
        "\n",
        "➕ **XGBoost:**\n",
        "\n",
        "* Also strong performer\n",
        "\n",
        "* More control via hyperparameters\n",
        "\n",
        "* Requires manual handling of categorical variables (via one-hot or label encoding)\n",
        "\n",
        "* Requires explicit imputation\n",
        "\n",
        "* Use if you need maximum tuning flexibility or feature interaction insights.\n",
        "\n",
        "➖ **AdaBoost:**\n",
        "\n",
        "* Older, simpler algorithm\n",
        "\n",
        "* Less effective with noisy data or missing values\n",
        "\n",
        "* Doesn’t handle categorical features or imbalance well\n",
        "\n",
        "* Usually outperformed by CatBoost and XGBoost\n",
        "\n",
        "* Not recommended for this problem.\n",
        "\n",
        "**6. Model Training (CatBoost)**\n",
        "\n",
        "    from catboost import CatBoostClassifier\n",
        "\n",
        "    model = CatBoostClassifier(\n",
        "        iterations=1000,\n",
        "        learning_rate=0.05,\n",
        "        depth=6,\n",
        "        cat_features=cat_feature_indices,  # pass list of categorical column indices\n",
        "        loss_function='Logloss',\n",
        "        eval_metric='AUC',\n",
        "        class_weights=[1, 5],  # example if default class is minority\n",
        "        random_seed=42,\n",
        "        early_stopping_rounds=50,\n",
        "        verbose=100\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train, eval_set=(X_val, y_val))\n",
        "\n",
        "\n",
        "**7. Evaluation**\n",
        "\n",
        "* AUC-ROC: Good for class imbalance\n",
        "\n",
        "* F1-score, Recall: Important to capture defaulters\n",
        "\n",
        "* Precision-Recall curve\n",
        "\n",
        "* Confusion matrix\n",
        "\n",
        "* Evaluate on both validation and test sets\n",
        "\n",
        "\n",
        "\n",
        "**8. Feature Importance & Interpretation**\n",
        "\n",
        "* Use CatBoost’s built-in feature importance\n",
        "\n",
        "* Use SHAP values for better interpretability\n",
        "\n",
        "**9. Iteration & Hyperparameter Tuning**\n",
        "\n",
        "* Use Optuna, GridSearchCV, or RandomSearch\n",
        "\n",
        "* Key parameters: iterations, depth, learning_rate, l2_leaf_reg, class_weights\n",
        "\n",
        "**10. Deployment & Monitoring**\n",
        "\n",
        "* Export model using model.save_model()\n",
        "\n",
        "* Serve via API (e.g., FastAPI, Flask)\n",
        "\n",
        "* Monitor:\n",
        "\n",
        "  * Data drift\n",
        "\n",
        "  * Performance drop\n",
        "\n",
        "  * Retraining pipeline triggers\n"
      ],
      "metadata": {
        "id": "lGwlvsiK6pcK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameter tuning strategy**\n",
        "\n",
        "**Step-by-Step Hyperparameter Tuning Strategy Using Boosting Techniques**\n",
        "\n",
        "**1. Data Preprocessing**\n",
        "\n",
        "* Handle missing values (or use model that supports them natively, like CatBoost)\n",
        "\n",
        "* Encode categorical features (CatBoost: automatic, XGBoost: manual)\n",
        "\n",
        "* Balance the classes:\n",
        "\n",
        " * Use class_weights or scale_pos_weight\n",
        "\n",
        " * Or resample: SMOTE/undersampling\n",
        "\n",
        "* Train-validation-test split (e.g., 70-15-15 or with stratification)\n",
        "\n",
        "**2. Objective of Hyperparameter Tuning**\n",
        "\n",
        "* Optimize predictive performance (e.g., AUC-ROC, F1, Recall) on imbalanced classification\n",
        "\n",
        "* Avoid overfitting/underfitting\n",
        "\n",
        "* Find best set of hyperparameters with minimal human trial-and-error\n",
        "\n",
        "**3. Hyperparameter Tuning Strategy**\n",
        "\n",
        "**Recommended Search Method: Bayesian Optimization (via Optuna)**\n",
        "\n",
        "* More efficient than grid/random search — it learns from past iterations to propose better next ones.\n",
        "\n",
        "**4. Key Hyperparameters to Tune (for CatBoost)**\n",
        "\n",
        "**Hyperparameter**........................................\t**Description**..........................................\t**Typical Range**\n",
        "\n",
        "iterations................................\tMax number of trees.........................................\t100–2000\n",
        "\n",
        "learning_rate....................................\tStep size shrinkage.......................................\t0.01–0.3\n",
        "\n",
        "depth.........................................\tTree depth...............................\t4–10\n",
        "\n",
        "l2_leaf_reg...................................\tL2 regularization...............................\t1–10\n",
        "\n",
        "bagging_temperature....................................\tRandomness in sampling........................................\t0–1\n",
        "\n",
        "random_strength.................................\tRandomness in splits..........................................\t1–20\n",
        "\n",
        "class_weights......................................\tImbalance adjustment.....................................\te.g., {0: 1, 1: 5}\n",
        "\n",
        "early_stopping_rounds........................\tStops training early if no improvement...................................\t50–200\n",
        "\n",
        "* CatBoost automatically handles categorical features and missing values, simplifying tuning.\n",
        "\n",
        "**5. Implementing Hyperparameter Tuning with Optuna (Example)**\n",
        "\n",
        "    import optuna\n",
        "    from catboost import CatBoostClassifier\n",
        "    from sklearn.metrics import roc_auc_score\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    # Split your dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Categorical feature indices (for CatBoost)\n",
        "    cat_features = [i for i, col in enumerate(X.columns) if X[col].dtype == 'object']\n",
        "\n",
        "    def objective(trial):\n",
        "        param = {\n",
        "            \"iterations\": trial.suggest_int(\"iterations\", 300, 1000),\n",
        "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
        "            \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
        "            \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1, 10),\n",
        "            \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0, 1),\n",
        "            \"random_strength\": trial.suggest_float(\"random_strength\", 1, 20),\n",
        "            \"loss_function\": \"Logloss\",\n",
        "            \"eval_metric\": \"AUC\",\n",
        "            \"cat_features\": cat_features,\n",
        "            \"verbose\": 0,\n",
        "            \"random_seed\": 42,\n",
        "            \"class_weights\": [1, 5],  # assume class 1 is minority\n",
        "            \"early_stopping_rounds\": 50\n",
        "        }\n",
        "    \n",
        "        model = CatBoostClassifier(**param)\n",
        "        model.fit(X_train, y_train, eval_set=(X_val, y_val), use_best_model=True)\n",
        "    \n",
        "        preds = model.predict_proba(X_val)[:, 1]\n",
        "        auc = roc_auc_score(y_val, preds)\n",
        "        return auc\n",
        "\n",
        "    # Run the optimization\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=50)\n",
        "\n",
        "    print(\"Best hyperparameters:\", study.best_params)\n",
        "\n",
        "\n",
        "**6. Final Model Training**\n",
        "\n",
        "Once tuning is complete:\n",
        "\n",
        "* Train the model on full training + validation set using study.best_params\n",
        "\n",
        "* Evaluate on the test set using AUC, F1, confusion matrix\n",
        "\n",
        "**7. Evaluate with Robust Metrics**\n",
        "\n",
        "For imbalanced data:\n",
        "\n",
        "* **AUC-ROC:** To assess general discriminative power\n",
        "\n",
        "* **Precision-Recall AUC:** For better insight on minority class\n",
        "\n",
        "* **F1-score / Recall:** Especially if catching defaulters is more critical\n",
        "\n",
        "**8. Optional: Cross-Validation**\n",
        "\n",
        "* Use Stratified K-Fold Cross Validation (e.g., 5-fold) to get stable estimates across splits during tuning:\n",
        "\n",
        "      from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "      cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n"
      ],
      "metadata": {
        "id": "buh7gNAtmSyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation metrics you'd choose and why**\n",
        "\n",
        "**Step-by-Step: Choosing Evaluation Metrics**\n",
        "\n",
        "**Step 1: Understand the Business Objective**\n",
        "\n",
        "* Predict loan default (binary classification: default = 1, non-default = 0)\n",
        "\n",
        "* Goal: Minimize risk by correctly identifying high-risk (defaulting) customers.\n",
        "\n",
        "* False Negatives (FN) are much worse than False Positives (FP)\n",
        "\n",
        "**Step 2: Recognize the Data Challenge**\n",
        "\n",
        "* Imbalanced data: Most customers repay their loans → accuracy will be misleading.\n",
        "\n",
        "* You want metrics that:\n",
        "\n",
        " * Focus on the minority class (default = 1)\n",
        "\n",
        " * Give insights into both detection quality and model discrimination\n",
        "\n",
        "**Step 3: Use Metrics Suitable for Imbalanced Classification**\n",
        "\n",
        "* **Recall (Sensitivity):** Measures how many actual defaulters you correctly caught. Critical in reducing missed defaulters.\n",
        "\n",
        "* **Precision:**\n",
        "Tells you how many predicted defaulters were actually correct. Helps avoid flagging too many good borrowers.\n",
        "\n",
        "* **F1 Score:**\n",
        "Harmonic mean of precision and recall. Good balance between catching defaulters and not over-flagging.\n",
        "\n",
        "* **ROC-AUC:**\n",
        "Measures model's ability to separate classes regardless of threshold. Good general measure of performance.\n",
        "\n",
        "* **PR-AUC:**\n",
        "More informative than ROC-AUC on highly imbalanced data. Focuses on the performance for the positive class (defaults).\n",
        "\n",
        "* **Confusion Matrix:**\n",
        "Gives raw counts of TP, TN, FP, FN — helps visualize trade-offs.\n",
        "\n",
        "**Step 4: Use Custom Evaluation During Model Tuning**\n",
        "\n",
        "* When training boosting models (e.g., CatBoost, XGBoost, LightGBM), pass your preferred metrics during training:\n",
        "\n",
        "**For CatBoost:**\n",
        "\n",
        "    model = CatBoostClassifier(\n",
        "        eval_metric='F1',  # or 'AUC', 'Precision', 'Recall'\n",
        "        custom_metric=['Precision', 'Recall', 'AUC'],\n",
        "        ...\n",
        "    )\n",
        "\n",
        "\n",
        "**For XGBoost:**\n",
        "\n",
        "    model = XGBClassifier(\n",
        "        scale_pos_weight=ratio,\n",
        "        eval_metric='aucpr',  # or 'auc', 'logloss'\n",
        "    )\n",
        "\n",
        "**Step 5: Threshold Tuning (Optional but Recommended)**\n",
        "\n",
        "* After training:\n",
        "\n",
        "* Models output probabilities, not labels.\n",
        "\n",
        "* You can tune the decision threshold (default is 0.5) to optimize recall, precision, or F1 depending on your business risk tolerance.\n",
        "\n",
        "      from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "      precision, recall, thresholds = precision_recall_curve(y_test, y_probs)\n",
        "      # Choose threshold where recall is high and precision is acceptable\n",
        "\n",
        "**Step 6: Post-Evaluation Considerations**\n",
        "\n",
        "* Use SHAP values or feature importance to explain false negatives.\n",
        "\n",
        "* Track model performance over time with a dashboard (e.g., recall drift, precision decay)."
      ],
      "metadata": {
        "id": "8lJcBydFqhvH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How the business would benefit from your model**\n",
        "\n",
        "**1. Proactively Reducing Loan Default Risk**\n",
        "\n",
        "**What your model does:**\n",
        "\n",
        "* Predicts the probability that a customer will default on a loan before it's approved.\n",
        "\n",
        "**Business benefit:**\n",
        "\n",
        "* Reduces financial losses by flagging high-risk borrowers.\n",
        "\n",
        "* Enables risk-based loan approval strategies (e.g., decline, reduce loan size, require collateral).\n",
        "\n",
        "**Example:** “Had this model been used last year, it could’ve prevented 30% of charge-offs from high-risk approvals.”\n",
        "\n",
        "**2. Personalized Credit Risk-Based Pricing**\n",
        "\n",
        "**What your model enables:**\n",
        "* Assign interest rates based on predicted default risk (risk-based pricing).\n",
        "\n",
        "**Business benefit:**\n",
        "\n",
        "* Maximizes profit by charging higher rates to higher-risk customers.\n",
        "\n",
        "* Keeps low-risk borrowers engaged with competitive offers.\n",
        "\n",
        "**Example:**\n",
        "* Safer borrowers pay less → better retention. Riskier borrowers either pay for the risk or are rejected.\n",
        "\n",
        "**. Improved Approval Rates Without Increasing Risk**\n",
        "\n",
        "**What your model enables:**\n",
        "* Identifies low-risk borrowers even among non-traditional applicants (e.g., limited credit history).\n",
        "\n",
        "**Business benefit:**\n",
        "\n",
        "* Increases loan approval volume safely.\n",
        "\n",
        "* Helps acquire new market segments (e.g., thin-file customers) using transaction behavior.\n",
        "\n",
        "**Example:**\n",
        "* Use transaction data to find safe customers even without a high credit score.\n",
        "\n",
        "**4. Smarter Collection Strategies**\n",
        "\n",
        "**What your model can also help with:**\n",
        "* Flagging borrowers likely to default early, so collections can start sooner or be prioritized.\n",
        "\n",
        "**Business benefit:**\n",
        "\n",
        "* Boosts collections efficiency by focusing efforts on high-risk, high-value accounts.\n",
        "\n",
        "* Reduces late-stage recovery costs.\n",
        "\n",
        "\n",
        "**5. Better Compliance and Transparency**\n",
        "\n",
        "**What your model supports:**\n",
        "* Using interpretable boosting models (like CatBoost + SHAP) allows for model transparency.\n",
        "\n",
        "**Business benefit:**\n",
        "\n",
        "* Easier to justify credit decisions to regulators and auditors.\n",
        "\n",
        "* Increased customer trust due to fairness and explainability.\n",
        "\n",
        "**Example:**\n",
        "* You can explain why a customer was rejected using feature importance (e.g., high debt ratio, low income stability).\n",
        "\n",
        "\n",
        "**6. Continuous Learning & Monitoring**\n",
        "\n",
        "**What your pipeline enables:**\n",
        "* Periodic retraining and monitoring to adapt to changing borrower behavior or macroeconomic conditions.\n",
        "\n",
        "**Business benefit:**\n",
        "\n",
        "* Maintains model accuracy over time.\n",
        "\n",
        "* Reduces exposure to shifts like economic downturns or new customer trends."
      ],
      "metadata": {
        "id": "kFY3bDU78jbW"
      }
    }
  ]
}